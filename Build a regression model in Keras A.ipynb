{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>79.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "      <td>61.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "      <td>40.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "      <td>41.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "      <td>44.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  Strength  \n",
       "0            1040.0           676.0   28     79.99  \n",
       "1            1055.0           676.0   28     61.89  \n",
       "2             932.0           594.0  270     40.27  \n",
       "3             932.0           594.0  365     41.05  \n",
       "4             978.4           825.5  360     44.30  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data = pd.read_csv('https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0101EN/labs/data/concrete_data.csv')\n",
    "concrete_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1030, 9)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "      <th>Strength</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>1030.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>281.167864</td>\n",
       "      <td>73.895825</td>\n",
       "      <td>54.188350</td>\n",
       "      <td>181.567282</td>\n",
       "      <td>6.204660</td>\n",
       "      <td>972.918932</td>\n",
       "      <td>773.580485</td>\n",
       "      <td>45.662136</td>\n",
       "      <td>35.817961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>104.506364</td>\n",
       "      <td>86.279342</td>\n",
       "      <td>63.997004</td>\n",
       "      <td>21.354219</td>\n",
       "      <td>5.973841</td>\n",
       "      <td>77.753954</td>\n",
       "      <td>80.175980</td>\n",
       "      <td>63.169912</td>\n",
       "      <td>16.705742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>102.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>121.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>801.000000</td>\n",
       "      <td>594.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>192.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>164.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>932.000000</td>\n",
       "      <td>730.950000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>23.710000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>272.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>968.000000</td>\n",
       "      <td>779.500000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>34.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>142.950000</td>\n",
       "      <td>118.300000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1029.400000</td>\n",
       "      <td>824.000000</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>46.135000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>540.000000</td>\n",
       "      <td>359.400000</td>\n",
       "      <td>200.100000</td>\n",
       "      <td>247.000000</td>\n",
       "      <td>32.200000</td>\n",
       "      <td>1145.000000</td>\n",
       "      <td>992.600000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>82.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cement  Blast Furnace Slag      Fly Ash        Water  \\\n",
       "count  1030.000000         1030.000000  1030.000000  1030.000000   \n",
       "mean    281.167864           73.895825    54.188350   181.567282   \n",
       "std     104.506364           86.279342    63.997004    21.354219   \n",
       "min     102.000000            0.000000     0.000000   121.800000   \n",
       "25%     192.375000            0.000000     0.000000   164.900000   \n",
       "50%     272.900000           22.000000     0.000000   185.000000   \n",
       "75%     350.000000          142.950000   118.300000   192.000000   \n",
       "max     540.000000          359.400000   200.100000   247.000000   \n",
       "\n",
       "       Superplasticizer  Coarse Aggregate  Fine Aggregate          Age  \\\n",
       "count       1030.000000       1030.000000     1030.000000  1030.000000   \n",
       "mean           6.204660        972.918932      773.580485    45.662136   \n",
       "std            5.973841         77.753954       80.175980    63.169912   \n",
       "min            0.000000        801.000000      594.000000     1.000000   \n",
       "25%            0.000000        932.000000      730.950000     7.000000   \n",
       "50%            6.400000        968.000000      779.500000    28.000000   \n",
       "75%           10.200000       1029.400000      824.000000    56.000000   \n",
       "max           32.200000       1145.000000      992.600000   365.000000   \n",
       "\n",
       "          Strength  \n",
       "count  1030.000000  \n",
       "mean     35.817961  \n",
       "std      16.705742  \n",
       "min       2.330000  \n",
       "25%      23.710000  \n",
       "50%      34.445000  \n",
       "75%      46.135000  \n",
       "max      82.600000  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concrete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data_columns = concrete_data.columns\n",
    "\n",
    "predictors = concrete_data[concrete_data_columns[concrete_data_columns != 'Strength']] # all columns except Strength\n",
    "target = concrete_data['Strength'] # Strength column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement</th>\n",
       "      <th>Blast Furnace Slag</th>\n",
       "      <th>Fly Ash</th>\n",
       "      <th>Water</th>\n",
       "      <th>Superplasticizer</th>\n",
       "      <th>Coarse Aggregate</th>\n",
       "      <th>Fine Aggregate</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.5</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198.6</td>\n",
       "      <td>132.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>978.4</td>\n",
       "      <td>825.5</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement  Blast Furnace Slag  Fly Ash  Water  Superplasticizer  \\\n",
       "0   540.0                 0.0      0.0  162.0               2.5   \n",
       "1   540.0                 0.0      0.0  162.0               2.5   \n",
       "2   332.5               142.5      0.0  228.0               0.0   \n",
       "3   332.5               142.5      0.0  228.0               0.0   \n",
       "4   198.6               132.4      0.0  192.0               0.0   \n",
       "\n",
       "   Coarse Aggregate  Fine Aggregate  Age  \n",
       "0            1040.0           676.0   28  \n",
       "1            1055.0           676.0   28  \n",
       "2             932.0           594.0  270  \n",
       "3             932.0           594.0  365  \n",
       "4             978.4           825.5  360  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    79.99\n",
       "1    61.89\n",
       "2    40.27\n",
       "3    41.05\n",
       "4    44.30\n",
       "Name: Strength, dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.2.0-cp37-cp37m-macosx_10_11_x86_64.whl (175.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 175.3 MB 6.8 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "  Downloading tensorboard-2.2.2-py3-none-any.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 14.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.2.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 4.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: six>=1.12.0 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.14.0)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "  Downloading tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454 kB)\n",
      "\u001b[K     |████████████████████████████████| 454 kB 14.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-preprocessing>=1.1.0\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 3.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading absl-py-0.9.0.tar.gz (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.4.1)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 169 kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.18.1)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.12.2-cp37-cp37m-macosx_10_9_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 16.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.30.0-cp37-cp37m-macosx_10_9_x86_64.whl (2.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.8 MB 10.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<2.11.0,>=2.10.0 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n",
      "Collecting astunparse==1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: pyyaml in ./opt/anaconda3/lib/python3.7/site-packages (from keras) (5.3)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (46.0.0.post20200309)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.1-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.2.2-py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 2.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (2.22.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 16.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.19.2-py2.py3-none-any.whl (91 kB)\n",
      "\u001b[K     |████████████████████████████████| 91 kB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in ./opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow) (1.0.0)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in ./opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in ./opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in ./opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow) (1.25.8)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting rsa<5,>=3.1.4; python_version >= \"3\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 13.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 13.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in ./opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow) (2.2.0)\n",
      "Collecting pyasn1>=0.1.3\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 1.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: termcolor, absl-py\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=ec567c65de96c162a90c26385620b488c82cdd045156b68bbcb1f6633d0eb598\n",
      "  Stored in directory: /Users/ericrautenbach/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "  Building wheel for absl-py (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for absl-py: filename=absl_py-0.9.0-py3-none-any.whl size=121931 sha256=b8834140e829a3c5b418a75b3300b4e9533dc580e249b394f4548434da23807b\n",
      "  Stored in directory: /Users/ericrautenbach/Library/Caches/pip/wheels/cc/af/1a/498a24d0730ef484019e007bb9e8cef3ac00311a672c049a3e\n",
      "Successfully built termcolor absl-py\n",
      "Installing collected packages: grpcio, absl-py, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, markdown, protobuf, tensorboard-plugin-wit, tensorboard, opt-einsum, tensorflow-estimator, keras-preprocessing, termcolor, google-pasta, gast, astunparse, tensorflow, keras\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define regression model\n",
    "def regression_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, activation='relu', input_shape=(n_cols,)))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "model = regression_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 909.1179 - val_loss: 376.4665\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 409.0100 - val_loss: 315.5823\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 316.0340 - val_loss: 306.4229\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 261.3146 - val_loss: 280.1573\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 228.7477 - val_loss: 258.8842\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 205.6863 - val_loss: 230.7663\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 190.7206 - val_loss: 208.4672\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 176.9921 - val_loss: 189.6630\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 165.7040 - val_loss: 171.3652\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 160.0639 - val_loss: 161.1145\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 156.1677 - val_loss: 143.1388\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 143.5581 - val_loss: 135.6977\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 138.4729 - val_loss: 125.0667\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 136.4249 - val_loss: 117.5500\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 131.5335 - val_loss: 119.0828\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 125.9948 - val_loss: 107.4665\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 121.9505 - val_loss: 104.8006\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 119.3421 - val_loss: 101.5159\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 118.0876 - val_loss: 101.6983\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 115.1740 - val_loss: 99.6664\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 116.8016 - val_loss: 96.2120\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 111.0253 - val_loss: 97.0694\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 111.4061 - val_loss: 93.9067\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 109.0262 - val_loss: 92.5066\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 109.6800 - val_loss: 98.1697\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 108.3663 - val_loss: 92.8334\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 109.6709 - val_loss: 97.7741\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 104.9357 - val_loss: 91.6683\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 104.9984 - val_loss: 91.3535\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 103.5464 - val_loss: 91.9662\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 104.0917 - val_loss: 95.9292\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 101.1763 - val_loss: 91.7342\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 102.2806 - val_loss: 93.2767\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 100.1433 - val_loss: 93.5808\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 100.6599 - val_loss: 105.7905\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 97.4767 - val_loss: 103.7839\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 98.2895 - val_loss: 93.3469\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 95.9334 - val_loss: 98.1987\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 98.6998 - val_loss: 102.6698\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 96.5686 - val_loss: 91.9569\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 98.1986 - val_loss: 105.8429\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 95.1865 - val_loss: 94.5307\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 97.5725 - val_loss: 96.5200\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 96.2577 - val_loss: 92.8044\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 94.7718 - val_loss: 92.2215\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 93.1144 - val_loss: 95.7046\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 93.1738 - val_loss: 94.1730\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 93.1603 - val_loss: 91.6824\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 92.8718 - val_loss: 91.2963\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 91.6690 - val_loss: 93.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc95fdfa3d0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(predictors, target, validation_split=0.3, epochs=50, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.convolutional import Conv2D # to add convolutional layers\n",
    "from keras.layers.convolutional import MaxPooling2D # to add pooling layers\n",
    "from keras.layers import Flatten # to flatten data for fully connected layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 - 0s - loss: 93.7575 - val_loss: 89.4163\n",
      "Epoch 2/50\n",
      "4/4 - 0s - loss: 92.5386 - val_loss: 85.8398\n",
      "Epoch 3/50\n",
      "4/4 - 0s - loss: 92.2720 - val_loss: 86.0424\n",
      "Epoch 4/50\n",
      "4/4 - 0s - loss: 92.2169 - val_loss: 86.2060\n",
      "Epoch 5/50\n",
      "4/4 - 0s - loss: 91.3817 - val_loss: 85.8996\n",
      "Epoch 6/50\n",
      "4/4 - 0s - loss: 91.0260 - val_loss: 84.9900\n",
      "Epoch 7/50\n",
      "4/4 - 0s - loss: 91.4299 - val_loss: 84.7360\n",
      "Epoch 8/50\n",
      "4/4 - 0s - loss: 90.3528 - val_loss: 85.7673\n",
      "Epoch 9/50\n",
      "4/4 - 0s - loss: 90.8738 - val_loss: 85.2947\n",
      "Epoch 10/50\n",
      "4/4 - 0s - loss: 90.7619 - val_loss: 84.3201\n",
      "Epoch 11/50\n",
      "4/4 - 0s - loss: 90.5066 - val_loss: 84.6456\n",
      "Epoch 12/50\n",
      "4/4 - 0s - loss: 90.0482 - val_loss: 84.5314\n",
      "Epoch 13/50\n",
      "4/4 - 0s - loss: 90.0168 - val_loss: 84.0241\n",
      "Epoch 14/50\n",
      "4/4 - 0s - loss: 90.1217 - val_loss: 84.5248\n",
      "Epoch 15/50\n",
      "4/4 - 0s - loss: 89.7498 - val_loss: 84.0160\n",
      "Epoch 16/50\n",
      "4/4 - 0s - loss: 90.1459 - val_loss: 83.6674\n",
      "Epoch 17/50\n",
      "4/4 - 0s - loss: 89.5335 - val_loss: 84.5717\n",
      "Epoch 18/50\n",
      "4/4 - 0s - loss: 89.8828 - val_loss: 84.4382\n",
      "Epoch 19/50\n",
      "4/4 - 0s - loss: 89.5424 - val_loss: 83.5763\n",
      "Epoch 20/50\n",
      "4/4 - 0s - loss: 89.1230 - val_loss: 83.8642\n",
      "Epoch 21/50\n",
      "4/4 - 0s - loss: 89.1769 - val_loss: 83.8787\n",
      "Epoch 22/50\n",
      "4/4 - 0s - loss: 89.3842 - val_loss: 83.9599\n",
      "Epoch 23/50\n",
      "4/4 - 0s - loss: 89.3048 - val_loss: 82.9789\n",
      "Epoch 24/50\n",
      "4/4 - 0s - loss: 88.9354 - val_loss: 83.4078\n",
      "Epoch 25/50\n",
      "4/4 - 0s - loss: 88.7921 - val_loss: 84.3111\n",
      "Epoch 26/50\n",
      "4/4 - 0s - loss: 88.8279 - val_loss: 82.9736\n",
      "Epoch 27/50\n",
      "4/4 - 0s - loss: 88.5839 - val_loss: 82.8021\n",
      "Epoch 28/50\n",
      "4/4 - 0s - loss: 88.4439 - val_loss: 83.3206\n",
      "Epoch 29/50\n",
      "4/4 - 0s - loss: 88.6339 - val_loss: 83.2070\n",
      "Epoch 30/50\n",
      "4/4 - 0s - loss: 88.3985 - val_loss: 82.4518\n",
      "Epoch 31/50\n",
      "4/4 - 0s - loss: 88.4260 - val_loss: 82.9066\n",
      "Epoch 32/50\n",
      "4/4 - 0s - loss: 88.4636 - val_loss: 83.9965\n",
      "Epoch 33/50\n",
      "4/4 - 0s - loss: 88.2641 - val_loss: 82.3942\n",
      "Epoch 34/50\n",
      "4/4 - 0s - loss: 88.1825 - val_loss: 82.2983\n",
      "Epoch 35/50\n",
      "4/4 - 0s - loss: 88.1068 - val_loss: 83.2616\n",
      "Epoch 36/50\n",
      "4/4 - 0s - loss: 88.1894 - val_loss: 82.7227\n",
      "Epoch 37/50\n",
      "4/4 - 0s - loss: 88.8233 - val_loss: 82.0737\n",
      "Epoch 38/50\n",
      "4/4 - 0s - loss: 88.2722 - val_loss: 83.7976\n",
      "Epoch 39/50\n",
      "4/4 - 0s - loss: 88.0466 - val_loss: 82.2521\n",
      "Epoch 40/50\n",
      "4/4 - 0s - loss: 87.7235 - val_loss: 82.0562\n",
      "Epoch 41/50\n",
      "4/4 - 0s - loss: 87.7733 - val_loss: 82.3730\n",
      "Epoch 42/50\n",
      "4/4 - 0s - loss: 87.5879 - val_loss: 82.2901\n",
      "Epoch 43/50\n",
      "4/4 - 0s - loss: 87.6059 - val_loss: 82.0298\n",
      "Epoch 44/50\n",
      "4/4 - 0s - loss: 87.5395 - val_loss: 82.4221\n",
      "Epoch 45/50\n",
      "4/4 - 0s - loss: 87.4833 - val_loss: 82.1362\n",
      "Epoch 46/50\n",
      "4/4 - 0s - loss: 87.3205 - val_loss: 82.3903\n",
      "Epoch 47/50\n",
      "4/4 - 0s - loss: 87.3176 - val_loss: 82.3793\n",
      "Epoch 48/50\n",
      "4/4 - 0s - loss: 87.3362 - val_loss: 81.9832\n",
      "Epoch 49/50\n",
      "4/4 - 0s - loss: 87.2752 - val_loss: 82.3077\n",
      "Epoch 50/50\n",
      "4/4 - 0s - loss: 87.1934 - val_loss: 81.9900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fc967266510>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=0)\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=200, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.98997422513267"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = model.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 87.6235\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 85.8804\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 86.7072\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 83.7580\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 87.8944\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 84.9844\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 86.0854\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 82.4137\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 83.2088\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 83.1910\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 82.4104\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 83.4670\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 85.2751\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 88.2335\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 81.9761\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 81.9844\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 82.4473\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 83.2640\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 82.1861\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 82.2623\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 81.2974\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 82.2639\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 80.6663\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 84.7491\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 84.8096\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 82.8113\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 80.8913\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 83.5780\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 80.7299\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 82.0226\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 81.8357\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 82.5356\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 80.4661\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 83.1029\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 82.1904\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 80.3150\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 81.6929\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 82.7081\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 81.9774\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 80.1611\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 83.0251\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 80.4269\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 81.0041\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 79.6221\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 80.9817\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 80.3401\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 85.4040\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 80.1581\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 81.7419\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 80.5675\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 85.5032\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 85.7898\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 83.2620\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 79.2140\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 79.4518\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 81.4926\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 81.0871\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 79.3868\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 79.9155\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 84.6178\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 80.5200\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 79.6133\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 80.4289\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 81.8352\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 79.4556\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 81.4107\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 79.9560\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 79.3378\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 84.4692\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 85.6918\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 82.6025\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 84.6186\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 80.4681\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 81.9759\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 82.0555\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 82.9987\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 83.3836\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 81.1342\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 79.2127\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 78.3435\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 83.9251\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 78.6330\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 78.1701\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 80.0163\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 81.0429\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 79.0198\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 77.5391\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 80.6116\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 78.4371\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 80.3618\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 78.9458\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 80.0530\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 82.8856\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 81.5206\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 79.9463\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 80.8451\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 86.5488\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 79.4340\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 78.3684\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 79.7081\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 79.6792\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 80.4007\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 80.8515\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 78.4934\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 78.6322\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 81.1171\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 82.5196\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 81.9532\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 80.1910\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 80.5096\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 78.8993\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 82.9911\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 80.8706\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 80.3123\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 80.0782\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 79.9969\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 90.4335\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 80.9349\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 78.6364\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 82.0688\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 79.6035\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 79.8973\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 82.7201\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 78.4660\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 78.4554\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 78.0735\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 79.5933\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 80.8545\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 79.6660\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 78.8221\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 79.4381\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 77.2626\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 78.1640\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 79.4664\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 82.4583\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 78.1965\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 83.1025\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 80.8590\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 77.7108\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 79.5814\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 81.5948\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 79.0936\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 77.9050\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 77.0905\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 83.7186\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 79.5298\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 81.4853\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 78.4863\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 80.6911\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 77.8968\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 79.5034\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 80.6567\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 78.4748\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 79.5713\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 78.0934\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 77.6672\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 79.1090\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 83.4973\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 79.0134\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 79.1968\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 77.8402\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 76.9330\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 78.0233\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 78.0585\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 77.9040\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 77.7136\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 80.5400\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 82.0856\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 77.3862\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 77.1576\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 77.5415\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 79.8846\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 76.6132\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 79.3814\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 76.8353\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 77.5161\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 78.9075\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 77.4099\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 78.5612\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 78.3162\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 77.6367\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 78.2654\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 78.7709\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 81.2920\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 79.0702\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 80.0777\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 82.7381\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 79.8231\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 77.9877\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 77.8677\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 77.3730\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 78.8100\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 78.2472\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 77.2690\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 77.0218\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 77.4904\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 83.5114\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 79.5640\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 81.6610\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 76.9829\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 81.3902\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 77.7840\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 80.2889\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 78.2388\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 81.5402\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 79.1542\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 78.8378\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 78.3264\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 76.9084\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 78.1730\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 76.4490\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 78.0782\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 79.3753\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 78.5704\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 81.0520\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 80.1861\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 77.1934\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 76.7241\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 76.8741\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 77.3495\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 81.3088\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 80.2504\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 74.2454\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 72.7588\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 72.5221\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 72.5602\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 76.4332\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 75.4744\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 72.3094\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 73.3523\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 72.9730\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 71.0717\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 73.0824\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 72.0878\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 71.6615\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 70.3888\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 71.1707\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 70.5131\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 70.7100\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 76.4362\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 70.4031\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 73.3864\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 69.3651\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 70.0041\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 71.2497\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 71.1240\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 69.2586\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 70.2931\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 69.4517\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 71.6199\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 70.6330\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 69.2662\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 71.0124\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 69.2511\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 68.7341\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 69.1739\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 68.1246\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 68.2719\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 68.3801\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 67.5286\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 68.0512\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 68.0958\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 68.1804\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 67.1871\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 67.7870\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 67.5180\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 67.9649\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 66.6761\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 66.2949\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 66.6612\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 67.6271\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 67.3075\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 65.1648\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 65.3827\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 66.3240\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 65.4302\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 66.2164\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 66.0832\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 65.4719\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 64.5604\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 63.9220\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 64.9075\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 63.8703\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 65.1085\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 63.8360\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 64.3709\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 65.7939\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 62.2610\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 64.5464\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 66.1081\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 69.8836\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 63.6660\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 62.9750\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 63.9309\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 62.1269\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 61.7042\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 63.0923\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 68.6207\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 62.8178\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 61.3761\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 63.0248\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 62.0802\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 62.7501\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 61.4172\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 61.2546\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 62.8394\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 60.9435\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 61.9492\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 64.2638\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 61.3156\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 61.0931\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 62.0806\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 61.7734\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 60.9929\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 60.4277\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 60.8558\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 64.7307\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 60.3995\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 60.0060\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 60.1136\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 59.8452\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 60.9440\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 59.8825\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 59.1669\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 60.0592\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 60.3602\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 63.0476\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 59.6309\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 61.7918\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 59.5037\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 58.5235\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 59.2117\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 58.5716\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 58.7059\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 60.9821\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 59.7736\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 59.1041\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 58.5710\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 59.0075\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 57.9172\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 57.5114\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 58.9625\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 58.0757\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 58.9825\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 61.6314\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 58.9496\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 61.5339\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 60.4723\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 58.3870\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 63.8535\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 63.6006\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 57.9934\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 56.2824\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 56.4992\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 58.8874\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 57.1286\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 57.8524\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 60.1004\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 59.1130\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 60.9639\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 57.4687\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 56.6740\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 55.4285\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 56.9861\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 62.0964\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 56.3276\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 56.2417\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 57.5698\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 56.2428\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 56.3272\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 55.2722\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 55.9533\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 54.8605\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 55.3600\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 54.4772\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 56.8253\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 56.8940\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 55.6554\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 54.4423\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 54.2674\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 55.2985\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 54.4306\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 54.1703\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 54.7355\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 55.1770\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 53.7543\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 55.9188\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 55.7362\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 55.4879\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 56.4598\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 55.7623\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 53.4792\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 53.6771\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 53.5964\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 56.8413\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 54.0892\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 52.6341\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 53.0367\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 52.4394\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 53.5951\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 53.9361\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 54.6801\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 52.5885\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 52.0153\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 52.5721\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 52.1765\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 54.5192\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 52.4018\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 52.2495\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 52.4995\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 51.3489\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 51.9192\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 51.7281\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 52.3802\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 52.1209\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 51.3003\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 51.6326\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 51.6611\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 52.8235\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 51.2673\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 53.3288\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 50.7010\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 50.6277\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 50.7075\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 52.3402\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 51.6063\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 51.2440\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 50.5155\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 53.2534\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 54.8888\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 51.2862\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 50.3914\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 51.5506\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 50.9932\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 51.3225\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 49.9045\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 50.8955\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 50.0944\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 50.5489\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 50.9455\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 49.6558\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 49.9226\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 50.0023\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 52.3785\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 52.4477\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 50.7091\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 51.3055\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 49.1324\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 50.3760\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 51.0603\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 50.5867\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 48.3836\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 49.3823\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 48.1191\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 50.7191\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 50.1789\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 48.9379\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 47.7797\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 48.6064\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 49.6475\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 49.2626\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 49.4457\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 47.9675\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 49.1807\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 49.4559\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 50.5982\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 50.5505\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 52.3414\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 47.6209\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 47.2378\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 47.9507\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 48.4840\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 49.8252\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 48.8798\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 50.7755\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 48.9888\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 48.4068\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 47.0635\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 47.1314\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 51.1499\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 49.0475\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 46.6123\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 45.9845\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 52.9786\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 48.4379\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 46.4586\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 48.6620\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 46.3271\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 46.7360\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 46.3040\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 45.7748\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 45.7215\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 46.3043\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 46.5586\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 46.4036\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 46.9362\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 47.5542\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 48.0471\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 46.6241\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 47.9327\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 47.3124\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 45.8418\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 45.0955\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 45.3976\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 45.8163\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 47.0040\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 46.1210\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 45.8320\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 45.3209\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 45.9692\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 45.0460\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 45.1689\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 44.5984\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 46.5692\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 45.8885\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 45.6373\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 44.4663\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 44.9595\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 47.6094\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 47.3001\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 45.3458\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 44.8430\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 45.1135\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 45.7195\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 44.4227\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 45.0426\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 45.1500\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 44.9964\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 45.2025\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 45.0074\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 44.3129\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 45.9229\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 45.8288\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 47.5992\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 47.9161\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 46.4177\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 44.6641\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 44.2004\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 44.4084\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 44.9326\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 46.0363\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 44.8101\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 43.9804\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 44.4828\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 44.2889\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 44.2189\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 43.8272\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 45.2164\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 43.7005\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 43.7605\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 44.1505\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 43.9795\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 44.2370\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 45.3377\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 44.6778\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 44.2665\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 44.1113\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 45.2090\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 44.2932\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 43.2461\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 44.8442\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 44.2356\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 43.5484\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 44.5706\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 44.5217\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 45.2719\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 43.1201\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 43.2696\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 43.2967\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 43.7607\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 44.2687\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 44.2203\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 44.3917\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 44.4784\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 43.0908\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 43.6085\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 43.2508\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 43.2170\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 43.5502\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 42.9736\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 42.6972\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 44.5783\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 42.9498\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 43.9896\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 43.1391\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 44.6265\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 47.2484\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 43.4699\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 44.3513\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 42.8591\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 43.8930\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 43.0299\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 43.2595\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 43.2456\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 43.3523\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 43.7465\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 44.2478\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 45.4012\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 45.5892\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 43.7090\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 43.7957\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 43.5163\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 43.6908\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 43.9228\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 43.4621\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 44.2557\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 45.5334\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 44.6864\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 43.6699\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 42.4761\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 42.9713\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 43.5425\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 43.5546\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 43.2645\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 42.7034\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 44.5175\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 44.6684\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 42.7217\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 42.9333\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 44.0483\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 43.7131\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 44.2731\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 42.6655\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 42.8930\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 42.9001\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 42.4891\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 43.1400\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 42.5494\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 43.0336\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 43.1328\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 43.9461\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 42.5249\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 42.7894\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 42.5723\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 43.0739\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 42.6751\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 42.4066\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 43.9951\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 42.2954\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 43.1301\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 43.0092\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 43.0836\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 42.7403\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 43.1432\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 42.5459\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 42.4533\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 43.0774\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 45.1574\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 42.7438\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 42.2907\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 44.8167\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 43.5588\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 44.5065\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 43.0091\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 42.3207\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 44.6024\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 45.7614\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 42.2268\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 42.2423\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 41.9736\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 42.9120\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 42.0998\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 42.2223\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 41.9984\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 43.9385\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 44.2195\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 43.9879\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 43.2977\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 42.7171\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 42.3966\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 42.9166\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 41.4440\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 44.0550\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 43.7863\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 41.6142\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 41.2151\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 42.2718\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 41.6113\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 41.6360\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 41.5425\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 41.4570\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 42.6476\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 42.8556\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 43.0000\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 41.3781\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 45.6968\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 42.9530\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 42.0997\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.4494\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 42.5094\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 42.5525\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 44.6335\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 44.0918\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 41.7549\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 43.3860\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 42.0667\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 41.5636\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 42.1369\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 41.5190\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 43.1042\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 41.8409\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 42.1477\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 41.5915\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 42.2249\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 43.5648\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 41.8890\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 42.0125\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 43.3133\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 41.4328\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 41.3377\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 42.1242\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 42.2441\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 43.6267\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 42.2221\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 41.6632\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 41.3385\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 41.4907\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 41.6588\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 43.5495\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 41.0442\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 42.2243\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 42.4736\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 43.3746\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 42.2416\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 41.6164\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 43.1269\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 44.5462\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 43.4207\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 40.9165\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 41.4819\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 41.4556\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 40.9593\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 42.7451\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 40.7626\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 42.3021\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 41.4538\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 40.9151\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 41.8168\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.7087\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 42.7378\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 42.9847\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 41.9938\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 41.7183\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 40.8486\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 41.1145\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 41.9828\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 40.8139\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 42.7316\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 41.9161\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 42.1386\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 41.9434\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 40.8570\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 40.7712\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 42.6615\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 42.6309\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 41.4861\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 42.9068\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 42.9244\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 41.6494\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 40.9624\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 40.7429\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 40.8868\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 41.3676\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 40.9839\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 42.4516\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 42.7854\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 41.2990\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 41.2424\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.4393\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 41.7044\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 42.5764\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 40.7439\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 40.6718\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 40.7613\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 41.8814\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 41.3203\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 41.8552\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 41.4692\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 41.1109\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 40.8697\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 42.8919\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 42.7115\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 41.8332\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 40.9874\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 40.7690\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 41.0590\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 42.8337\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 43.2122\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.2443\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 41.4024\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 41.3917\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 40.6847\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 41.6206\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 43.2999\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.6590\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 41.3395\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 40.9603\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 42.5401\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.6822\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 41.0539\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 42.3210\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 40.7578\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 40.7738\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 41.1608\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 41.2592\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 40.7694\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 41.2367\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 42.7392\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 41.3884\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 41.9513\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 42.9174\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 43.3673\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 42.1702\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 40.8107\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 42.0284\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 41.3689\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 42.4833\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 40.7393\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.9643\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 42.2947\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 41.8847\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 41.2416\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 42.2624\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 40.2400\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 40.2336\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 40.4644\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 41.7308\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 40.3805\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 42.1648\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 41.1229\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 40.2677\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 41.3313\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 43.5444\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 40.3019\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 40.8154\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 40.0124\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 40.6882\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 40.1669\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.4904\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 40.0437\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 41.0227\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 40.1650\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.2852\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 41.2348\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.5021\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 41.3163\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 40.9048\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 40.2446\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.6353\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 40.4650\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 41.2113\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 41.3620\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 41.1283\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 42.3736\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 40.1306\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 40.3892\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 43.8431\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 42.9318\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 42.0459\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 40.7984\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 42.1265\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 41.8736\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 40.2785\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 40.5413\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 41.6741\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 41.2588\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 40.1262\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 40.0383\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.1411\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 40.2898\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 41.3035\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 41.6720\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 41.7914\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 41.1215\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 41.1013\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 40.5994\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 40.5721\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 40.3920\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 40.0780\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 40.2315\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 40.2057\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 41.2292\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 41.1756\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 41.8784\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 40.6952\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 41.7414\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 40.3188\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 41.2446\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 43.3155\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 41.5417\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 42.3212\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 40.8951\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.3183\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 39.9973\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.7431\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 40.7608\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.9623\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.9696\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.6806\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 40.8201\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 43.6703\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 42.2272\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 41.5686\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 40.2701\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 40.8467\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 40.0158\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 39.7536\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 41.7038\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 40.2546\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 40.6031\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.8911\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 40.1428\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 39.9955\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.8638\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 40.6174\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 42.1646\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.8635\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 39.6508\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.0982\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 40.3192\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 40.1832\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 40.2661\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 42.6106\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 41.3957\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 41.5085\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 41.0029\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 40.2787\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 40.9760\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 42.4684\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 40.3173\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 41.3803\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 40.6711\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 41.3353\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 39.9255\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 40.1344\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 39.7310\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 40.1194\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 42.3294\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 40.0935\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 39.8986\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 41.3082\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 40.2646\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.1000\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 39.7496\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.4207\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.7727\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 41.0595\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 41.7790\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.5064\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 41.8836\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 40.4715\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 39.7997\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 40.0354\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 40.0804\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 39.5857\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.5256\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 39.6202\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 39.6194\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 40.7418\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 40.3551\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 40.7866\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 41.5261\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 41.1771\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 42.2670\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 42.3393\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.5934\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 40.2043\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 40.2350\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 39.7651\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 39.8211\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 40.6064\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.9902\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 39.6515\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 39.7275\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 42.2552\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 42.7665\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 39.7671\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 42.0543\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 41.1841\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 40.6824\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 40.7805\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 41.2537\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 40.7111\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 40.9240\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 39.4574\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 40.9321\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 41.6625\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 41.8685\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.3808\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 40.3876\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 40.4725\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 40.8692\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 39.3988\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 39.3314\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 39.2859\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 40.6396\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 40.1008\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.7868\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.4821\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 40.1131\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 39.4687\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 41.6449\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 39.7628\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 40.3054\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 41.3447\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 40.4171\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 39.8037\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 41.9667\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 41.5752\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 39.8995\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.9829\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.8550\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 40.2788\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.6762\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 40.6957\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 40.2796\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 42.1148\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 39.6626\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 39.5086\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 40.1961\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 41.4373\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 40.4440\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 39.4760\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 42.4171\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 39.7597\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 40.1353\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 39.3299\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 39.5676\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.4678\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 39.4594\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 39.9156\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 40.0660\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 40.6730\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 38.9586\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 41.7292\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 44.9541\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 40.2807\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.9517\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 39.9782\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 40.3086\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 39.2391\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 39.1760\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 39.5679\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 40.5904\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.5393\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.6382\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 41.0851\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 40.0918\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.5542\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.8298\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 41.4269\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 40.6385\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 40.0116\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 39.3091\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 39.8009\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.6669\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 39.2962\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 39.4011\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 39.3697\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 39.2640\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.2756\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.3425\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 39.1226\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.1911\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 41.1311\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 41.0433\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 40.3432\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 39.3234\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 39.2850\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 38.8659\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 40.6525\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.3104\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 40.4828\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 41.9487\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 39.3771\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 39.1241\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 39.0934\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 40.1621\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 38.7111\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 39.1109\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.8106\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 40.1372\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 40.8331\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 39.0233\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 39.1326\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 39.8223\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 41.6307\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 39.6634\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 39.0313\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 40.0047\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 40.9396\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 39.0552\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.9693\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 40.4795\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 39.9263\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.4842\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.4690\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.5037\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.7370\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 42.4477\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 39.4524\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 41.3887\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 39.1119\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 39.4505\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 39.1227\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 40.2103\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 40.4755\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 40.9076\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 40.4510\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.7916\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.2544\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 38.9766\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.9328\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.3223\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 40.5610\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.2026\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.4471\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 40.7306\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.0310\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 39.1558\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 40.4367\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.4853\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 40.1416\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 38.6731\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 38.5213\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 39.7255\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 38.7971\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 39.0445\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.5170\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 39.8741\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.7978\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 38.6055\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 39.2311\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 39.7434\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.6114\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 39.0300\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 39.3277\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 40.8832\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 39.2677\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 39.8236\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 39.9864\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 39.4251\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 39.0482\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 38.9107\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 39.0346\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.0503\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 38.7916\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 38.6660\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.8731\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 39.5456\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.5672\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 39.4188\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 39.2118\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 38.5214\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 38.4260\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.0856\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 40.2784\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 39.4066\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 39.5054\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 39.8034\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.0614\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.6554\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 40.5842\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.0262\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.9967\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.9522\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 40.9266\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.7826\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 38.4654\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 39.0951\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.7576\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 41.5731\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 40.0527\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 38.2829\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 39.1425\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 40.0995\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 39.7674\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.2912\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 40.7345\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.8823\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.4045\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 39.0366\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 38.8672\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 40.7162\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 40.6679\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 38.8607\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.7259\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 39.2542\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 39.7135\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 38.5535\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.4763\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.9879\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 39.3279\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 38.9258\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 38.7744\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 41.9985\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 40.2362\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.2566\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 39.1299\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.7935\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.7697\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 38.9050\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 39.1333\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 39.0121\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 39.4585\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.6082\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.8464\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 39.0804\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.7319\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.4689\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.8873\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.9670\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.8875\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.7216\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 39.1679\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.8873\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.8353\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.1979\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 41.0918\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 38.8448\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 40.1422\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.0971\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.6362\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 39.9425\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 39.9949\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 38.6547\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 40.0088\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 39.7883\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.2106\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.8897\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 39.8774\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 39.4580\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 39.5923\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 38.7394\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.4865\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 38.1327\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.1660\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.6998\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 40.7348\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 39.1240\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.3577\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.8519\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 38.4148\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 39.7039\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 38.8766\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.7599\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.1315\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.7253\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.3011\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.8811\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 39.9712\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 39.1932\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.9704\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 38.1063\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 39.2131\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 38.1939\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 40.8097\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 40.2701\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.0253\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.5510\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.8548\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.5717\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.6987\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.2233\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.5806\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.1384\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 38.7046\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.3784\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.6783\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 41.4268\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.7830\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 38.1526\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 39.3157\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 38.3003\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 38.8759\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 38.6657\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 38.8471\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.9038\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.8636\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.0954\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.5172\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 39.2203\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 39.3182\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 38.0518\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.4158\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 38.2766\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.1975\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.0739\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 38.1655\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 38.7633\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 39.1921\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.6385\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 38.0451\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.9714\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.5912\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.4738\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.3224\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.6660\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.1526\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 40.3575\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.8657\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 38.3041\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 38.6781\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 38.1550\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.9415\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.7164\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.5263\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 38.0739\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.7076\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.4787\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.7353\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 38.3487\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.9274\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.2056\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.2167\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.8172\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 37.8709\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.0812\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 38.7852\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 39.4847\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.5697\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 38.5431\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.8559\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 40.7063\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 40.0862\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.9484\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 38.8644\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 41.1168\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.5623\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.6097\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.5132\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.7367\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.3853\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.4388\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.1820\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.4257\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 39.0969\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.8930\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 38.4274\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 38.6725\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 40.5495\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.7592\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.6387\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 40.0770\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 39.1514\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.2287\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.2281\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.3338\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 40.0485\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.9284\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.9845\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 38.4761\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 38.7966\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.7885\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 38.0349\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 38.7504\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 39.2931\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.8404\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.0929\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.8258\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 38.5405\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.7386\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.7163\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.8574\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 39.7029\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.3226\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 40.4371\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 39.3387\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.7877\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.8228\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.9274\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 38.4132\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 37.7618\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.5385\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.7538\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 38.8463\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 39.0131\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.2223\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.9666\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 39.2939\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.8701\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 39.1429\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 39.0895\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 39.2290\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.4322\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 38.2502\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.3322\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.9311\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.8031\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 38.9899\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 40.5606\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.5664\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 38.2358\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 38.1854\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 38.7215\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 39.1271\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 38.3419\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 38.1944\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.2864\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.6166\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.5980\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 39.8162\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 39.2480\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 39.6036\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 38.6602\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.5568\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.5798\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.7582\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.4008\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 39.4520\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 39.5956\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 40.9456\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.0500\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.7316\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.1507\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.8968\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 37.3074\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 39.0705\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.4467\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.7797\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.1038\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.4403\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 40.5482\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 38.9028\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 38.2186\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 40.4198\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 38.5598\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.5965\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.7505\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 39.3125\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.4559\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.8649\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.2432\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.8301\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 41.1997\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 39.9465\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.1238\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.7483\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 41.8205\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.9485\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.5965\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.0221\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.4538\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 39.6252\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.5928\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.2077\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.2393\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.8039\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.0568\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.1111\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.5755\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.6223\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 38.5153\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 38.2320\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.6815\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.6445\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.5462\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.6005\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.6377\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.3889\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 38.1138\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.1156\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.3621\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 39.9948\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 40.5290\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.7638\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 38.2378\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.4259\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.9194\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.9625\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.3133\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.6750\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 37.7431\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.1503\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.9900\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.5811\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.9582\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.1408\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.7593\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.6392\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.4034\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.7713\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.2049\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.8042\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.5965\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.4652\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.3289\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.3846\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.9839\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 39.1844\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.1484\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.0504\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 40.0078\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.5309\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 40.4254\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 40.0968\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 38.3365\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 39.3241\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.3411\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.7455\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.3683\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.5258\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.2150\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 39.4520\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 38.4242\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.9355\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.0282\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.3032\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.8285\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 38.6729\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 38.6641\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 38.4377\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.9993\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.5233\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 37.3013\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.6122\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 37.2704\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.5654\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 38.2007\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.7627\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.4818\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 38.1415\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.7072\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 39.1516\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 38.0847\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.3943\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 38.4021\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.1427\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 38.4480\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.4054\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.7692\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 39.1281\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.1964\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.7782\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.4921\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.6845\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.8233\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.2310\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.8738\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.6446\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 37.8880\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.4563\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.6587\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.2513\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.5641\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.8923\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.9479\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.7293\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 37.3019\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.7544\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.1311\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 38.0732\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.3186\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.9581\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.3740\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.7456\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.9499\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.0977\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.3784\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.3641\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.8254\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 39.0549\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.1708\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.3188\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.2254\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.9744\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.7986\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.3065\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 38.2705\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.0169\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.7955\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.9134\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.8588\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 38.2151\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 39.8286\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 38.2819\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.9725\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.6091\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 38.6312\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.8094\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.1115\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.7032\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.9338\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.8394\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.9918\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 39.5455\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 39.2906\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.1559\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.2682\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.8379\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.5296\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.3301\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.5876\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.7956\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.1297\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.9316\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 38.7878\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 38.1004\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.9588\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.6488\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.6392\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.4764\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 38.1141\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.6157\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.2286\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.5657\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.8008\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 40.3964\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 39.0866\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.0402\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.0053\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.0773\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.1106\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.1329\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.9569\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.3901\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 38.8975\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 41.0530\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.8099\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 38.1669\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.2274\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 38.5075\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 39.5411\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 40.6696\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.0976\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.8924\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.9886\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.9301\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.7770\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.1950\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.1063\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 38.1188\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.0620\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.3628\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.9859\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.6460\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.5775\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.9993\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.8274\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.8859\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 37.2547\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.9179\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.4922\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.5324\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.2218\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.2359\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.1136\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.8352\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.0564\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 40.5732\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.1863\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.8667\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.1802\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.6927\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.8624\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.2376\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.3043\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.5671\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.7555\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.8912\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.7586\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.0554\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.0657\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.5105\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 37.0135\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.4856\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.0095\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 37.5067\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.7188\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.0335\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 39.5477\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 40.3516\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 37.5467\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.9090\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.4547\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.1448\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 42.3893\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.9453\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.0424\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.2474\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.3124\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.0462\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.9514\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.3887\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.1162\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.3973\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.0410\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.9079\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.3079\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.9315\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.0706\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.3220\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 38.6942\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.3085\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.2665\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.9641\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.4335\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.3575\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 39.0622\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.3064\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.5352\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.5167\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.0838\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.1335\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.7564\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.1876\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.9031\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 38.2348\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.3815\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.6551\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.0861\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.6629\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 39.3221\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 36.9171\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.7936\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 39.6971\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.8568\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 37.0759\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.9775\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.2721\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.0740\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 39.8310\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.0104\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 39.0173\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.5236\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.4937\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.6361\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.6453\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.2370\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.8326\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.4746\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 38.0534\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.3874\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 38.6204\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.0875\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.7252\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.2110\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.2778\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.8180\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.1545\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.0079\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.3347\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.4215\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.7022\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.8050\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.2578\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.7095\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.6939\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.9104\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.1986\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 38.9778\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.8321\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.9235\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.7320\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.9290\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.0205\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.5770\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.0057\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.2669\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.9017\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.3535\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.5831\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 36.8018\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 36.8126\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.7288\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.0273\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.8867\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.9969\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.3759\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.2529\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.9810\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.0185\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 38.1811\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.8523\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.2039\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.7552\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.8671\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.2141\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.9782\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.1485\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.7073\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.6079\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.9300\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.6908\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.5289\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.4058\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.0964\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.1251\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.6541\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.8023\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.1118\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.9011\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.1238\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.0065\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.0994\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.4763\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.6235\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.4267\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.5607\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.2380\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.9848\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.8305\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.2244\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.6171\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.7030\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.7492\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.3094\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.4378\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.6547\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.9980\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 38.2609\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.6589\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.8380\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.5607\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 41.7212\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 38.5652\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 38.0148\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 37.9752\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 38.0954\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.3686\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.3782\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.0319\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 37.0205\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.7275\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.1012\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.6557\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.1282\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.8348\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.4257\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.1499\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.8008\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 41.5596\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.3879\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.3731\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.4637\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.3593\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 38.0423\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.6217\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.7725\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 38.1659\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 37.3587\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.8355\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.1552\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 38.6120\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 38.4384\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.9077\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.6807\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.4824\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.3120\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.4953\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.2587\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.5278\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 37.6727\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 38.4143\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 38.0518\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 39.1978\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.8859\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.7314\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.4926\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.4765\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.0104\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.5997\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 36.7776\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.8136\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 37.2720\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.9410\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.2262\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.2362\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.5613\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.4304\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.1492\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.8945\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 41.0182\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.1354\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.4254\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.2874\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.7121\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.7167\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.7804\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.4898\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 38.1257\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 38.6662\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.6521\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.3162\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.7724\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.4554\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.3561\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.7015\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.1227\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.5759\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.7113\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.2078\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.7949\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.3461\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.7034\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.3967\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.2516\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 38.3276\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.0278\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 38.4489\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.4815\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.4905\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.8448\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.8093\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.7038\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.6414\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 38.4864\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3768\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.1461\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.6177\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.5104\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.1905\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.9369\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.8647\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 37.4192\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.9060\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.2111\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.4356\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.2202\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.7171\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.2036\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.3687\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.0349\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.4593\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.0806\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.2483\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.1084\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 39.0197\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.4865\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.0274\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.6508\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.1880\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.1370\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.7968\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.6982\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.7813\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.6510\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.6946\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.6896\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.3848\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.3956\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.5651\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.3850\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.2208\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.0305\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.4685\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.3370\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.9743\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 37.3362\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.3257\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.3673\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 37.4506\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.6217\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.2707\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.0813\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.3868\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.0029\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.4834\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.0171\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.3716\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.6098\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.0374\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.3216\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.4415\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 37.0807\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.6505\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 38.3740\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.3885\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.4276\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.1581\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 38.8105\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.0100\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.8182\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.3009\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.2149\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.3287\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.0970\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.2422\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.6264\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 36.8909\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 37.4335\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 36.6194\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.1737\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.0263\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.9321\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.2057\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.8511\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.3802\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.4387\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.4588\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.4821\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.1391\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.6641\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.4241\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.7011\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.5720\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.1464\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.9363\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.6410\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.5005\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.9223\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.0785\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 38.7428\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.0230\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.7120\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.6379\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.6421\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3824\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.2864\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.2837\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.4976\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.2490\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 36.7702\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 36.6886\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 39.7687\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.5272\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 39.1030\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.4314\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.2522\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.0549\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.4788\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 36.4541\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.4095\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.5937\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.1717\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.4577\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 35.9502\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.2850\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.1767\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.1179\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 35.7847\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 36.7846\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.3969\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.5891\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 38.3290\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.4573\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.0720\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.4927\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.8099\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.3487\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.5783\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.3467\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.3044\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 35.8909\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 36.6621\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.9877\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.0901\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.2948\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.6067\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 36.1584\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.6213\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.2731\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.4325\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 35.7020\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.6716\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 35.8728\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.5796\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3504\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.8250\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.8612\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.3235\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.2757\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.3865\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.5269\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.6173\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 37.0919\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.7813\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 38.0467\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.3035\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.5069\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.9521\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 36.9955\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.3731\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.9999\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.1453\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.2620\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.9489\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.5317\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.5651\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.6047\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.8811\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 36.8492\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 35.6949\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.9096\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.3831\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 38.3468\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.3789\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.6605\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.0812\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.8780\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.3443\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.4450\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.8894\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.7865\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 36.6349\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 37.7603\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 39.0687\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.7932\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.5821\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.0306\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.4218\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.2436\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.5104\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 37.0983\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.4006\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 38.5736\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.1308\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.4987\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.5383\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.1118\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 37.3799\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.1728\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.4175\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 40.1767\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.1692\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.4834\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 37.3021\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.4612\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 35.8516\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.2006\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 35.8722\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 35.9418\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 35.9106\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.1498\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.5040\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.0166\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.9844\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.1341\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.0429\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 36.7834\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.0924\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 36.1412\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.0413\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.7438\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.1242\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.0497\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 35.9578\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.2063\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.3712\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.7980\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.4025\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 35.8150\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 36.2330\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.0310\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.4364\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 38.6788\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.3283\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.0238\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 37.2825\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 36.4119\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.1856\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.6798\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.2030\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.4183\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.6050\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 38.5937\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.3547\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3298\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.4584\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.4282\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 35.9320\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.5465\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.7083\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 38.0653\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 38.0717\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.2539\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.4024\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 37.0993\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.7034\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.0331\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.3054\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 35.6660\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.4924\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.5437\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.0942\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.2381\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.2615\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.4613\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 37.2594\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 35.9528\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.3010\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.1613\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 38.5674\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.0567\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.5699\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.5170\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.2497\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.5842\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.1507\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 37.7589\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 38.0558\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 37.2547\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 35.8666\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 37.0123\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 37.5822\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.4883\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 37.0297\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.6681\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.0203\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 35.8790\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 35.8829\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 35.8275\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.4004\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.0514\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.1776\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.4276\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.3628\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3136\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 37.6628\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.0726\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.2051\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.7885\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.8917\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 36.0997\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.2282\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 35.8408\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.2107\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 37.5526\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.7668\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.8082\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.6763\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.7804\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.6859\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 37.7874\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 38.2186\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 38.4552\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.3433\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.3391\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 39.2991\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 36.4162\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 35.5858\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 35.8240\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 35.7469\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.1182\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.5062\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 37.8541\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.9734\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 38.0522\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.4620\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.6094\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 37.6051\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.3301\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 35.8859\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.5889\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 35.8656\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.3071\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.1849\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.6205\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 37.9597\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.7715\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.1156\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 35.5708\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.1564\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 35.8645\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 37.5516\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 36.1862\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 35.9304\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 37.2258\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.1120\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.2739\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.5706\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.0651\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 36.1207\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 35.8584\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.3011\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.0584\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.0450\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.6739\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 40.1406\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.9621\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 35.7968\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 36.7469\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 35.7678\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.4150\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.0164\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 36.4009\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.0947\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.1932\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 40.7958\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 37.4837\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 35.7077\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.5583\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 35.9397\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.0037\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.0413\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.0733\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 37.0348\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 37.5954\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 35.8545\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 35.6530\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.5731\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 38.6888\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 38.2883\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.5165\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 36.5542\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.4436\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 35.7215\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 37.9863\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 38.2177\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.6687\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 37.4892\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 35.9450\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 35.6935\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.3036\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.2585\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 35.5406\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 37.1896\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.9096\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.9276\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.6345\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.8948\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.8201\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 37.9727\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 36.8322\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 35.9223\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 35.5271\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 36.5828\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.7510\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.5822\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.6347\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.5660\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 35.8449\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 37.0047\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 36.2604\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 35.8836\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 35.5855\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 35.6875\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 37.0998\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 36.4056\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 36.1120\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 35.8674\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 36.5395\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 37.7483\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.8556\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.3032\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.1774\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.6644\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 35.5171\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 36.6540\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.4984\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 35.8813\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 35.5228\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 35.5048\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.2612\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 36.0970\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.2280\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.4421\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 35.2442\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.4486\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 36.6765\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 35.9470\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 36.2709\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 35.9786\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.8653\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.1434\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 35.7004\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.7538\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 36.3662\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 38.1270\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.3118\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 35.9522\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 37.7819\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 38.4959\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.6516\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 35.9532\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.8105\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 37.7196\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 35.8715\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 36.4212\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 37.9309\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 37.0447\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 36.7740\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.7495\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 35.6265\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 36.3343\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 37.9055\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 37.1199\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 38.7853\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 35.6558\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 38.5186\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.6951\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.1545\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.6044\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.2101\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 37.5976\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.0925\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 35.3754\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.1019\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 35.6589\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 35.1905\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 36.4685\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 35.8166\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 37.4768\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.7502\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 36.1905\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.1482\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.4378\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.1632\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 36.3375\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 35.6578\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.0916\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 35.5293\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 35.9153\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 35.3772\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.3633\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 37.8884\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 35.9067\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 35.9168\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.1927\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 37.2341\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 36.7049\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 36.9808\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 36.0978\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.1117\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 35.8368\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.2278\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 35.8979\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 36.3100\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 37.9248\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.8626\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.2276\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 37.2247\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 36.2572\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 35.8943\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 37.5081\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 35.8487\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 35.5167\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 35.5934\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 35.8975\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 36.0457\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 36.0220\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.1080\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 35.7896\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 36.1711\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 36.6299\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 36.2547\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 36.0677\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 36.2348\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 37.4450\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 36.7762\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 39.6043\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 36.2550\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 35.7448\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 36.7024\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 35.7072\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.6205\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 35.7402\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 36.2149\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 35.9783\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.2229\n",
      "Epoch 1/50\n",
      "23/23 - 0s - loss: 36.1335\n",
      "Epoch 2/50\n",
      "23/23 - 0s - loss: 35.6498\n",
      "Epoch 3/50\n",
      "23/23 - 0s - loss: 36.1822\n",
      "Epoch 4/50\n",
      "23/23 - 0s - loss: 36.9852\n",
      "Epoch 5/50\n",
      "23/23 - 0s - loss: 36.2368\n",
      "Epoch 6/50\n",
      "23/23 - 0s - loss: 35.8726\n",
      "Epoch 7/50\n",
      "23/23 - 0s - loss: 36.3220\n",
      "Epoch 8/50\n",
      "23/23 - 0s - loss: 35.1137\n",
      "Epoch 9/50\n",
      "23/23 - 0s - loss: 36.0540\n",
      "Epoch 10/50\n",
      "23/23 - 0s - loss: 36.5271\n",
      "Epoch 11/50\n",
      "23/23 - 0s - loss: 35.6994\n",
      "Epoch 12/50\n",
      "23/23 - 0s - loss: 35.7462\n",
      "Epoch 13/50\n",
      "23/23 - 0s - loss: 35.7615\n",
      "Epoch 14/50\n",
      "23/23 - 0s - loss: 37.3593\n",
      "Epoch 15/50\n",
      "23/23 - 0s - loss: 36.2032\n",
      "Epoch 16/50\n",
      "23/23 - 0s - loss: 36.0392\n",
      "Epoch 17/50\n",
      "23/23 - 0s - loss: 35.7345\n",
      "Epoch 18/50\n",
      "23/23 - 0s - loss: 35.2378\n",
      "Epoch 19/50\n",
      "23/23 - 0s - loss: 35.9637\n",
      "Epoch 20/50\n",
      "23/23 - 0s - loss: 36.3128\n",
      "Epoch 21/50\n",
      "23/23 - 0s - loss: 36.1239\n",
      "Epoch 22/50\n",
      "23/23 - 0s - loss: 36.0666\n",
      "Epoch 23/50\n",
      "23/23 - 0s - loss: 35.5066\n",
      "Epoch 24/50\n",
      "23/23 - 0s - loss: 35.5264\n",
      "Epoch 25/50\n",
      "23/23 - 0s - loss: 35.5025\n",
      "Epoch 26/50\n",
      "23/23 - 0s - loss: 35.8491\n",
      "Epoch 27/50\n",
      "23/23 - 0s - loss: 36.6933\n",
      "Epoch 28/50\n",
      "23/23 - 0s - loss: 36.1215\n",
      "Epoch 29/50\n",
      "23/23 - 0s - loss: 35.5276\n",
      "Epoch 30/50\n",
      "23/23 - 0s - loss: 35.1360\n",
      "Epoch 31/50\n",
      "23/23 - 0s - loss: 35.8754\n",
      "Epoch 32/50\n",
      "23/23 - 0s - loss: 37.4862\n",
      "Epoch 33/50\n",
      "23/23 - 0s - loss: 36.3266\n",
      "Epoch 34/50\n",
      "23/23 - 0s - loss: 37.2077\n",
      "Epoch 35/50\n",
      "23/23 - 0s - loss: 35.9875\n",
      "Epoch 36/50\n",
      "23/23 - 0s - loss: 35.8926\n",
      "Epoch 37/50\n",
      "23/23 - 0s - loss: 35.8460\n",
      "Epoch 38/50\n",
      "23/23 - 0s - loss: 35.7487\n",
      "Epoch 39/50\n",
      "23/23 - 0s - loss: 35.8379\n",
      "Epoch 40/50\n",
      "23/23 - 0s - loss: 35.0620\n",
      "Epoch 41/50\n",
      "23/23 - 0s - loss: 35.6400\n",
      "Epoch 42/50\n",
      "23/23 - 0s - loss: 35.5805\n",
      "Epoch 43/50\n",
      "23/23 - 0s - loss: 35.1619\n",
      "Epoch 44/50\n",
      "23/23 - 0s - loss: 34.7588\n",
      "Epoch 45/50\n",
      "23/23 - 0s - loss: 34.7922\n",
      "Epoch 46/50\n",
      "23/23 - 0s - loss: 36.1923\n",
      "Epoch 47/50\n",
      "23/23 - 0s - loss: 36.2359\n",
      "Epoch 48/50\n",
      "23/23 - 0s - loss: 35.2684\n",
      "Epoch 49/50\n",
      "23/23 - 0s - loss: 37.8070\n",
      "Epoch 50/50\n",
      "23/23 - 0s - loss: 37.4314\n"
     ]
    }
   ],
   "source": [
    "mse_lst_norm = []\n",
    "for n in range(50):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.3, random_state=1)\n",
    "    model.fit(X_train, y_train, epochs=50, verbose=2)\n",
    "    y_hat = model.predict(X_test)\n",
    "    sse = mean_squared_error(y_test, y_hat)\n",
    "    mse_lst_norm.append(sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.194% (2.966%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "/Users/ericrautenbach/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate using Shuffle Split Cross Validation\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataframe = predictors\n",
    "array = dataframe.values\n",
    "X = array[:,0:7]\n",
    "Y = array[:,7]\n",
    "num_samples = 10\n",
    "test_size = 0.20\n",
    "num_instances = len(X)\n",
    "seed = 7\n",
    "kfold = model_selection.ShuffleSplit(n_splits=50, test_size=test_size, random_state=seed)\n",
    "model = LogisticRegression()\n",
    "results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean()*100.0, results.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
